{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Gradient Descent: Step Sizes - Lab\n", "\n", "## Introduction\n", "\n", "In this lab, you'll practice applying gradient descent.  As you know, gradient descent begins with an initial regression line and moves to a \"best fit\" regression line by changing values of $m$ and $b$ and evaluating the RSS.  So far, we have illustrated this technique by changing the values of $m$ and evaluating the RSS.  In this lab, you will work through applying this technique by changing the value of $b$ instead.  Let's get started.\n", "\n", "## Objectives\n", "\n", "You will be able to:\n", "\n", "- Use gradient descent to find the optimal parameters for a linear regression model\n", "- Describe how to use an RSS curve to find the optimal parameters for a linear regression model"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import sys\n", "import numpy as np\n", "np.set_printoptions(formatter={'float_kind':'{:f}'.format})\n", "import matplotlib.pyplot as plt"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Setting up Our Initial Regression Line\n", "\n", "Once again, we'll take a look at revenues (our data example), which looks like this:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["np.random.seed(225)\n", "\n", "x = np.random.rand(30, 1).reshape(30)\n", "y_randterm = np.random.normal(0,3,30)\n", "y = 3 + 50*x + y_randterm\n", "\n", "fig, ax = plt.subplots()\n", "ax.scatter(x, y, marker=\".\", c=\"b\")\n", "ax.set_xlabel(\"x\", fontsize=14)\n", "ax.set_ylabel(\"y\", fontsize=14)\n", "fig.suptitle(\"Revenues\");"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We can start with some values for an initial not-so-accurate regression line, $y = 43x + 12$."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def regression_formula(x):\n", "    return 43*x + 12"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We plot this line with the same data below:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["fig, ax = plt.subplots()\n", "ax.scatter(x, y, marker=\".\", c=\"b\")\n", "ax.plot(x, regression_formula(x), color=\"orange\", label=r'$y = 43x + 12$')\n", "ax.set_xlabel(\"x\", fontsize=14)\n", "ax.set_ylabel(\"y\", fontsize=14)\n", "fig.suptitle(\"Revenues\", fontsize=16)\n", "ax.legend();"]}, {"cell_type": "markdown", "metadata": {}, "source": ["As you can see, this line is near the data, but not quite right. Let's evaluate that more formally using RSS."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def errors(x_values, y_values, m, b):\n", "    y_line = (b + m*x_values)\n", "    return (y_values - y_line)\n", "\n", "def squared_errors(x_values, y_values, m, b):\n", "    return errors(x_values, y_values, m, b)**2\n", "\n", "def residual_sum_squares(x_values, y_values, m, b):\n", "    return sum(squared_errors(x_values, y_values, m, b))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now using the `residual_sum_squares`, function, we calculate the RSS to measure the accuracy of the regression line to our data.  Let's take another look at that function:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["residual_sum_squares(x, y , 43, 12)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["So, for a $b$ of 12, we are getting an RSS of 1117.8. Let's see if we can do better than that!"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Building a cost curve\n", "\n", "Now let's use the `residual_sum_squares` function to build a cost curve.  Keeping the $m$ value fixed at $43$, write a function called `rss_values`.  \n", "* `rss_values` passes our dataset with the `x_values` and `y_values` arguments.  \n", "* It also takes a list of values of $b$, and an initial $m$ value as arguments.  \n", "* It outputs a NumPy array with a first column of `b_values` and second column of `rss_values`. For example, this input:\n", "  ```python\n", "  rss_values(x, y, 43, [1, 2, 3])\n", "  ```\n", "  Should produce this output:\n", "  ```python\n", "  array([[1.000000, 1368.212664],\n", "       [2.000000, 1045.452004],\n", "       [3.000000, 782.691343]])\n", "  ```\n", "  Where 1, 2, and 3 are the b values an 1368.2, 1045.5 and 782.7 are the associated RSS values.\n", "  \n", "*Hint:* Check out `np.zeros` ([documentation here](https://numpy.org/doc/stable/reference/generated/numpy.zeros.html))."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Replace None with appropriate code\n", "def rss_values(x_values, y_values, m, b_values):\n", "    # Make a NumPy array to contain the data\n", "    None\n", "    \n", "    # Loop over all of the values in b_values\n", "    for idx, b_val in enumerate(b_values):\n", "        # Add the current b value and associated RSS to the\n", "        # NumPy array\n", "        None\n", "        None\n", "        \n", "    # Return the NumPy array\n", "    None"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Run this cell without changes\n", "example_rss = rss_values(x, y, 43, [1,2,3])\n", "\n", "# Should return a NumPy array\n", "assert type(example_rss) == np.ndarray\n", "\n", "# Specifically a 2D array\n", "assert example_rss.ndim == 2\n", "\n", "# The shape should match the number of b values passed in\n", "assert example_rss.shape == (3, 2)\n", "\n", "example_rss"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now let's make more of an attempt to find the actual best b value for our `x` and `y` data.\n", "\n", "Make an array `b_val` that contains values between 0 and 14 with steps of 0.5.\n", "\n", "*Hint:* Check out `np.arange` ([documentation here](https://numpy.org/doc/stable/reference/generated/numpy.arange.html))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Replace None with appropriate code\n", "b_val = None\n", "b_val"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now use your `rss_values` function to find the RSS values for each value in `b_val`. Continue to use the m value of 43.\n", "\n", "We have included code to print out the resulting table."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Replace None with appropriate code\n", "bval_rss = None\n", "np.savetxt(sys.stdout, bval_rss, '%16.2f') # this line is to round your result, which will make things look nicer."]}, {"cell_type": "markdown", "metadata": {}, "source": ["This represents our cost curve!\n", "\n", "Let's plot this out using a a line chart."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["fig, ax = plt.subplots(figsize=(10,7))\n", "ax.plot(bval_rss[:,0], bval_rss[:,1])\n", "ax.set_xlabel(r'$b$ values', fontsize=14)\n", "ax.set_ylabel(\"RSS\", fontsize=14)\n", "fig.suptitle(\"RSS with Changes to Intercept\", fontsize=16);"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Looking at the Slope of Our Cost Curve\n", "\n", "In this section, we'll work up to building a gradient descent function that automatically changes our step size.  To get you started, we'll provide a function called `slope_at` that calculates the slope of the cost curve at a given point on the cost curve.\n", "\n", "Use the `slope_at` function for b-values 3 and 6 (continuing to use an m of 43)."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def slope_at(x_values, y_values, m, b):\n", "    delta = .001\n", "    base_rss = residual_sum_squares(x_values, y_values, m, b)\n", "    delta_rss = residual_sum_squares(x_values, y_values, m, b + delta)\n", "    numerator = delta_rss - base_rss\n", "    slope = numerator/delta\n", "    return slope"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Use slope_at for b value 3\n", "\n", "# -232.73066022784406"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Use slope_at for b value 6\n", "\n", "# -52.73066022772355"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The `slope_at` function takes in our dataset, and returns the slope of the cost curve at that point.  So the numbers -232.73 and -52.73 reflect the slopes at the cost curve when b is 3 and 6 respectively.\n", "\n", "Below, we plot these on the cost curve."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Setting up to repeat the same process for 3 and 6\n", "# (You can change these values to see other tangent lines)\n", "b_vals = [3, 6]\n", "\n", "def plot_slope_at_b_vals(x, y, m, b_vals, bval_rss):\n", "    # Find the slope at each of these values\n", "    slopes = [slope_at(x, y, m, b) for b in b_vals]\n", "    # Find the RSS at each of these values\n", "    rss_values = [residual_sum_squares(x, y, m, b) for b in b_vals]\n", "\n", "    # Calculate the actual x and y locations for plotting\n", "    x_values = [np.linspace(b-1, b+1, 100) for b in b_vals]\n", "    y_values = [rss_values[i] + slopes[i]*(x_values[i] - b) for i, b in enumerate(b_vals)]\n", "    \n", "    # Plotting the same RSS curve as before\n", "    fig, ax = plt.subplots(figsize=(10,7))\n", "    ax.plot(bval_rss[:,0], bval_rss[:,1])\n", "    ax.set_xlabel(r'$b$ values', fontsize=14)\n", "    ax.set_ylabel(\"RSS\", fontsize=14)\n", "\n", "    # Adding tangent lines for the selected b values\n", "    for i in range(len(b_vals)):\n", "        ax.plot(x_values[i], y_values[i], label=f\"slope={round(slopes[i], 2)}\", linewidth=3)\n", "\n", "    ax.legend(loc='upper right', fontsize='large')\n", "    fig.suptitle(f\"RSS with Intercepts {[round(b, 3) for b in b_vals]} Highlighted\", fontsize=16)\n", "    \n", "plot_slope_at_b_vals(x, y, 43, b_vals, bval_rss)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Let's look at the above graph.  When the curve is steeper and downwards at $b = 3$, the slope is around -232.73.  And at $b = 6$ with our cost curve becoming flatter, our slope is around -52.73. \n", "\n", "## Moving Towards Gradient Descent\n", "\n", "Now that we are familiar with our `slope_at` function and how it calculates the slope of our cost curve at a given point, we can begin to use that function with our gradient descent procedure.\n", "\n", "Remember that gradient descent works by starting at a regression line with values m, and b, which corresponds to a point on our cost curve.  Then we alter our m or b value (here, the b value) by looking to the slope of the cost curve at that point.  Then we look to the slope of the cost curve at the new b value to indicate the size and direction of the next step.\n", "\n", "So now let's write a function called `updated_b`.  The function will tell us the step size and direction to move along our cost curve.  The `updated_b` function takes as arguments an initial value of $b$, a learning rate, and the `slope` of the cost curve at that value of $m$.  Its return value is the next value of `b` that it calculates."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def updated_b(b, learning_rate, cost_curve_slope):\n", "    pass"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Test out your function below. Each time we update `current_b` and step a little closer to the optimal value."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["b_vals = []\n", "\n", "current_b = 3\n", "b_vals.append(current_b)\n", "\n", "current_cost_slope = slope_at(x, y, 43, current_b)\n", "new_b = updated_b(current_b, .01, current_cost_slope)\n", "print(f\"\"\"\n", "Current b: {round(current_b, 3)}\n", "Cost slope for current b: {round(current_cost_slope, 3)}\n", "Updated b: {round(new_b, 3)}\n", "\"\"\")\n", "# Current b: 3\n", "# Cost slope for current b: -232.731\n", "# Updated b: 5.327"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["current_b = new_b\n", "b_vals.append(current_b)\n", "\n", "current_cost_slope = slope_at(x, y, 43, current_b)\n", "new_b = updated_b(current_b, .01, current_cost_slope)\n", "print(f\"\"\"\n", "Current b: {round(current_b, 3)}\n", "Cost slope for current b: {round(current_cost_slope, 3)}\n", "Updated b: {round(new_b, 3)}\n", "\"\"\")\n", "# Current b: 5.327\n", "# Cost slope for current b: -93.092\n", "# Updated b: 6.258"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["current_b = new_b\n", "b_vals.append(current_b)\n", "\n", "current_cost_slope = slope_at(x, y, 43, current_b)\n", "new_b = updated_b(current_b, .01, current_cost_slope)\n", "print(f\"\"\"\n", "Current b: {round(current_b, 3)}\n", "Cost slope for current b: {round(current_cost_slope, 3)}\n", "Updated b: {round(new_b, 3)}\n", "\"\"\")\n", "# Current b: 6.258\n", "# Cost slope for current b: -37.237\n", "# Updated b: 6.631"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["current_b = new_b\n", "b_vals.append(current_b)\n", "\n", "current_cost_slope = slope_at(x, y, 43, current_b)\n", "new_b = updated_b(current_b, .01, current_cost_slope)\n", "print(f\"\"\"\n", "Current b: {round(current_b, 3)}\n", "Cost slope for current b: {round(current_cost_slope, 3)}\n", "Updated b: {round(new_b, 3)}\n", "\"\"\")\n", "# Current b: 6.631\n", "# Cost slope for current b: -14.895\n", "# Updated b: 6.78"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Take a careful look at how we use the `updated_b` function.  By using our updated value of $b$ we are quickly converging towards an optimal value of $b$.\n", "\n", "In the cell below, we plot each of these b values and their associated cost curve slopes. Note how the tangent lines get closer together as the steps approach the minimum."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plot_slope_at_b_vals(x, y, 43, b_vals, bval_rss)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We can visualize the actual lines created by those b values against the data like this:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["fig, ax = plt.subplots(figsize=(10,7))\n", "ax.scatter(x, y, marker=\".\", c=\"b\")\n", "colors = ['#ff7f0e', '#2ca02c', '#d62728', '#9467bd']\n", "for i, b in enumerate(b_vals):\n", "    ax.plot(x, x*43 + b, color=colors[i], label=f'$y = 43x + {round(b, 3)}$', linewidth=3)\n", "ax.set_xlabel(\"x\", fontsize=14)\n", "ax.set_ylabel(\"y\", fontsize=14)\n", "fig.suptitle(\"Revenues\", fontsize=16)\n", "ax.legend();"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now let's write another function called `gradient_descent`.  The inputs of the function are `x_values`, `y_values`, `steps`, the `m` we are holding constant, the `learning_rate`, and the `current_b` that we are looking at.  The `steps` arguments represent the number of steps the function will take before the function stops.  We can get a sense of the return value in the cell below.  It is a list of dictionaries, with each dictionary having a key of the current `b` value, the `slope` of the cost curve at that `b` value, and the `rss` at that `b` value."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def gradient_descent(x_values, y_values, steps, current_b, learning_rate, m):\n", "    pass"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["descent_steps = gradient_descent(x, y, 15, 0, learning_rate = .005, m = 43)\n", "descent_steps\n", "\n", "#[{'b': 0, 'rss': 1750.97, 'slope': -412.73},\n", "# {'b': 2.063653301142949, 'rss': 1026.94, 'slope': -288.91},\n", "# {'b': 3.5082106119386935, 'rss': 672.15, 'slope': -202.24},\n", "# {'b': 4.519400729495828, 'rss': 498.29, 'slope': -141.57},\n", "# {'b': 5.2272338117862205, 'rss': 413.1, 'slope': -99.1},\n", "# {'b': 5.72271696938941, 'rss': 371.35, 'slope': -69.37},\n", "# {'b': 6.06955517971187, 'rss': 350.88, 'slope': -48.56},\n", "# {'b': 6.312341926937677, 'rss': 340.86, 'slope': -33.99},\n", "# {'b': 6.482292649996282, 'rss': 335.94, 'slope': -23.79},\n", "# {'b': 6.601258156136964, 'rss': 333.53, 'slope': -16.66},\n", "# {'b': 6.684534010435641, 'rss': 332.35, 'slope': -11.66},\n", "# {'b': 6.742827108444089, 'rss': 331.77, 'slope': -8.16},\n", "# {'b': 6.7836322770506285, 'rss': 331.49, 'slope': -5.71},\n", "# {'b': 6.812195895074922, 'rss': 331.35, 'slope': -4.0},\n", "# {'b': 6.832190427692808, 'rss': 331.28, 'slope': -2.8}]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Looking at our b-values, you get a pretty good idea of how our gradient descent function works.  It starts far away with $b = 0$, and the step size is relatively large, as is the slope of the cost curve.  As the $b$ value updates such that it approaches a minimum of the RSS, the slope of the cost curve and the size of each step both decrease.\n", "\n", "Compared to the initial RSS of 1117.8 when $b$ was 12, we are down to 331.3!\n", "\n", "Remember that each of these steps indicates a change in our regression line's slope value towards a \"fit\" that more accurately matches our dataset.  Let's plot the final regression line as found before, with $m=43$ and $b=6.83$"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# plot the final result here"]}, {"cell_type": "code", "execution_count": 20, "metadata": {}, "outputs": [{"data": {"image/png": "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\n", "text/plain": ["<Figure size 432x288 with 1 Axes>"]}, "metadata": {"needs_background": "light"}, "output_type": "display_data"}], "source": ["fig, ax = plt.subplots()\n", "ax.scatter(x, y, marker=\".\", c=\"b\")\n", "ax.plot(x, x*43 + 6.83, color='#17becf', label=f'$y = 43x + 6.83$')\n", "ax.set_xlabel(\"x\", fontsize=14)\n", "ax.set_ylabel(\"y\", fontsize=14)\n", "fig.suptitle(\"Revenues\", fontsize=16)\n", "ax.legend();"]}, {"cell_type": "markdown", "metadata": {}, "source": ["As you can see, this final intercept value of around $b=6.8$ matches our data much better than the previous guess of 12. Remember that the slope was kept constant. You can see that lifting the slope upwards could probably even lead to a better fit!\n", "\n", "## Summary\n", "\n", "In this lesson, we learned some more about gradient descent.  We saw how gradient descent allows our function to improve to a regression line that better matches our data.  We see how to change our regression line, by looking at the Residual Sum of Squares related to the current regression line. We update our regression line by looking at the rate of change of our RSS as we adjust our regression line in the right direction -- that is, the slope of our cost curve.  The larger the magnitude of our rate of change (or slope of our cost curve) the larger our step size.  This way, we take larger steps the further away we are from our minimizing our RSS, and take smaller steps as we converge towards our minimum RSS. "]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.5"}}, "nbformat": 4, "nbformat_minor": 2}